# 上下文处理

## 2025-11-19

* 随着本地维护上下文消息，每次调用API导致token线性增长，尤其是工具使用上的增长直接导致token的消耗按工具调用倍数增长，于此同时我注意到即便LLM支持同时调用多个工具，但实际运行时，LLM倾向于单独调用工具，以至于成功调用工具后需要重新发起新的API请求，这不仅大量消耗token的同时还会造成不必要的浪费
* 对于上下文的长文本输入，LLM注意力机制会出现["中间迷失"](https://arxiv.org/html/2508.05128v1)和["注意力盆地"](https://arxiv.org/html/2508.05128v1)

# 模型选择

## 2025-11-20

我发现`GLM-4.6`似乎钟情于调用工具，以至于一次简单改动居然打掉十万token

## 2025-11-22

我对`GLM-4.6`,`kimi-k2-0905-preview`,`qwen3-max`进行测试，发现一个问题，似乎不同的模型对工具调用的偏好是不一样，在实际使用中`GLM-4.6`偏爱调用工具，而其他两款模型似乎不太愿意主动调用工具，并且检查具体实现并测试后这两个模型能够调用工具，只是为什么相比`GLM-4.6`调用工具差这么多



## 需要优化的目标

### 上下文优化

- [ ] 目前上下文体系还是“最简单、但浪费”的实现——它能保证语义正确（不丢信息），但从“噪音控制”和“token 成本”来看，是明显需要优化的
- [ ] 目前diff片段使用的是修改前的上下文+修改后的上下文进行拼接，显然这是不合理的，仅需要将修改的diff和公用的上下文进行拼接

### 会话体系

- [ ] 仅支持工具调用间的上下文传递，未支持多轮 Agent 调度时的全局状态管理。