# 🥳欢迎使用

DeltaConverge是一个以Agent为核心架构的AI Code Review系统，采用多Agent协同工作的方式，洞悉PR存在的问题。本系统历时两个月深度打磨，持续迭代，旨在提高代码审查的准确性。但时间紧，任务重，从零开发到提交作品的过程中，诸多方面或许存在一些意想不到的情况，但在正式提交前，还是做了诸多测试确保系统的稳定性，希望在有限的时间里做到尽善尽美。

## DeltaConverge

每一次新功能，新的优化的诞生都始于一次**背离**，代码的价值不在于孤立存在，而在于回归与融合当分支发起 Pull Request，它必须经过审核，与主支的代码、团队的规范、产品的愿景**Converge**（汇聚合一）。这不是简单的合并，而是一次需要被理解、被检验、被接纳的集成。

DeltaConverge，捕获 PR 中每一个细微的 Delta，以清晰的分析引导它完成 Converge 的使命。

> *差异在此汇聚，代码因此完整*

<details>
<summary>开始（快速入手）</summary>
<h2>开始</h2>
<p>如何部署使用DeltaConverge已经在<a href="../README.md">README.md</a>中详细介绍，接下来讨论如何快速入手，即便在开发时尽可能降低上手门槛，并对交互进行优化，但我始终站在开发者视角进行优化，难免有些地方做的不妥，如有感到奇怪，不自然的操作，或意外BUG，那定然是开发时未能够及时发现，倘若遇到这些问题，还望海涵，不过我能确保整个使用体验能够达到一个相对舒服的状态，因为开发时对大部分功能和设计都进行过层层打磨</p>
<blockquote>下面这部分按照前端左侧目录自顶向下，仅供参考，旨在快速上手本项目前端</blockquote>
<h3>仪表盘</h3>
<p>前端中提供两个选择项目的途径</p>
<ul>
<li>代码审查左上角</li>
<li>仪表盘左侧第一栏</li>
</ul>
<p>在这里能够非常方便的进行项目选择，并且提供系统选择器丰富不同选择逻辑，需要注意的是，选择的项目<strong>必须</strong>要有<code>.git</code>，因为这是系统能够正常运行的基础，目前暂未对选择不存在<code>.git</code>的文件夹做过多处理，始终相信用户选择的是正确的路径。</p>
<p>当选择路径后，前端会在仪表盘中第四栏中调整审查本项目需要使用的扫描器，其中包含扫描语言和扫描器名称。同时在底部，用户可以提前进行项目意图分析，此时分析Agent将会对整个项目进行分析，这个分析报告后续将作为业务意图向后续链路进行传递，非常建议此时进行分析，在得到分析报告后，可以进行审查，并手动修改这份报告。</p>
<p>如果没有在仪表盘中进行业务意图分析，在开始任务审查时，内核会检测是否存在对本项目的业务意图分析，如果没有缓存文件，那么在开始审查前也会进行一次业务意图分析。</p>
<p>在仪表盘中第三栏，提供密钥填写和模型配置的设置，这些数据均来源于内核中的文件，前端只是进行简单的读写功能。</p>
<h3>代码审查</h3>
<p>这部分是开始审查的核心区域，在底部中，有自动运行和静态分析两个配置</p>
<p>自动运行：通常来说，并不需要开启自动运行，进行代码审查并不应该需要过多工具和权限，所以默认的七个工具为白名单自动放行，自动运行只针对这七个工具以外的工具，不过暂时只有这七个工具，后续可添加更多工具</p>
<blockquote><code>echo_tool</code>为测试工具，不属于这个范畴</blockquote>
<p>静态分析：经过考虑，静态分析作为旁路运行，不影响主链路运行，启用后，会调用扫描器对受影响的文件进行扫描，在最后结果输出的时候和审查报告的结果进行统一，在大多数情况下可以打开。</p>
<h3>代码变更</h3>
<p>这里能够看到diff内容，并且可以根据自己喜好进行调整视图</p>
<h3>历史记录</h3>
<p>这里能够回看对某一项目的审查记录，对于不同的项目会话进行了区分，所以能够看到的会话都属于选择的项目审查报告，历史记录中的数据都进行保留，可以看到每一个细节，目前并没有对会话名称自动命名进行一个很好的设计，所以选择会话记录需要进行些甄别，如果有需要关注的会话记录可以进行重命名，这样可以明显区分不同会话</p>
<h3>规则优化</h3>
<p>如何进行规则优化，原理是什么，怎么使用，这些问题比较繁琐，因此在设计的时候，将此类话题设计为使用手册，可以在<code>参考提示</code>中<code>?</code>查看，当鼠标靠近，它会指引你去点击</p>
<h3>调试与缓存</h3>
<p>这部分属于项目中早期但始终没有优化的部分，也算是整个项目为数不多没有优化的地方，考虑到整个项目能够直接查看日志文件进行问题分析，因此这部分属于一个尴尬的情况，后续也并没有添加更多检测相关数据的功能，因此可以忽略这部分。</p>
<h3>设置</h3>
<p>这里可以配置整个内核的参数，尽量减少魔法数字的出现的情况，也使得整个开发得以有效进行，值得注意的是，里面大多数参数并不需要动，因为在早期内核使用硬编码，即便当初预留接口，现在对接后可能仍存在部分技术债，不确定修改参数是否全部能够正常运行，不过可以放心的是，系统并不会因此而罢工。</p>
</details>

# 开发历程

这是我第一次真正意义上完成的项目，从写下第一行代码到写下最后一篇文档，总历时`74`天，耗时`500h`以上，课余时间几乎全身心投入项目的功能开发，细节优化，架构设计等打磨中。开发本项目之前，曾对py有过基础的学习，能够编写简单的脚本完成简单的任务，并尝试使用py完成过几个小工具或对AI有一定的兴趣进行研究，诸如`解密教务系统，定期爬取课表`，`使用docx库，实现合同自动生成`，`使用py进行数据集增强，微调大模型`，`使用wallpaper创建优雅的网页，进行壁纸自定义`，`利用AI实现自动化答题`等等，以上的经历让我对开发有了一定的了解，能够有勇气选择技术赛道，并做出一个自认为不错的作品。

得益于AI的迅速发展，让信息差间距迅速拉低，让写代码变成一个简单的事情，对于开发者，只需要进行规划，制定需求，AI能够完成大多数任务和需求，这是我能够迅速成长的催化剂，也是我能够完成完成本项目的根本原因，倘若仅凭借手搓代码，那是断然不可能完成本项目的。

在整个项目的开发中，各种功能的实现，BUG修复，细节优化等`99%`的代码是由AI协助完成的，并且在前期的架构方面设计也是和AI进行深入讨论，细致分析得出的，可以说，这是一个高度化的AI定制项目。窃以为，使用AI进行开发是当今开发过程的趋势所向，并且未来初级代码工作一定被LLM取代，市面上有以编程能力著称的`Claude Opus 4.5`，前端能力超然的`Gemini 3 pro`,个人认为推理能力超强的`Gpt-5.2 X-Hight Reasoning`御三家，这三款模型能够进行配合，解决大多数编程任务，在一定程度上这些模型比肩专家水平，但是所有的LLM都有一个通病，或者说生来就有的问题——Transformer架构导致LLM永远无法主动思考。

## 开发理念

> 如果开发者都没有清晰的思路，整个项目只会在一次次零散的对话中，被AI执行的面目全非

在正式创建文件夹之前，首先要确定，这个项目是什么，怎么写，架构是什么的，采用什么方式进行。

在项目初期，因为没有明确这个项目是什么，而贸然动手写代码导致[作品](https://github.com/Gust-feng/AI_Code_Review)偏离题意

所幸，在开发的过程中多次回看任务目标，琢磨这个项目核心需求是什么，在开发第二天便发觉项目偏离题意，及时调整

在此后的一周里，为了确保项目能够切合题意，把握重点，因此刻意放缓项目的开发进度，仔细研读任务目标，并和AI进行深入交流探讨项目开发重点，将初步想法确定下来

<details>
<summary>初步规划</summary>
<img src="存档/image/AGENT.webp" alt="初步规划" />
</details>
开发初期，借助LLM，整个过程都很顺利，遇到的问题都能借助AI解决，一切有条不紊的进行，我开始享受这种感觉，直到进行测试的时候发现，系统效果不尽人意，一大根本原因是上下文问题，输入的上下文太杂，以至于前期的系统，可以简单的认为只是一个简单的反馈系统。有意思的是，项目开发过程中的所有测试都是基于项目本身的修改进行的，当系统进行优化后，使用的系统进行一次“自检”，一来观察修改的功能或Bug是否正确，二来观察系统能否根据修改后的代码检测出些问题，以便观察审查效果。

我发觉，项目的开发不是这么简单，起码不是像我此前制作的那些脚本或程序一样能一次到位或打补丁就能解决的，它势必需要经过多轮迭代，层层测试才能进一步成长。而这个过程，需要清晰的思路和持续的执行力，其难度对开发者而言不可谓不小。

### 模块化处理

就像搭积木一样，将整个项目模块化，这是在开发前得出得结论。无论是项目开发还是功能分类都应该遵循模块化原则，这一点贯彻整个项目的理念，以至于在项目中期创建的前端原型`main.js`，`style.css`写到6700，4500行也依然将前端完完整整的剥离成模块文件。

当然，在任何设计开始之初始终遵循模块化理念，从新建文件夹开始构建的底层，即便不同厂商的API格式都兼容open ai格式，但仍然将不同的api抽象为统一的私有格式，使得上层应用层无需关心传回来的API格式是什么样，只需要消费已经规划好的API格式便可，后来构建不同Agent，链路传递，扫描器等等都遵循这个理念。

### 持续迭代

这是一个从零开始的项目，即便已经确定整体架构，也不可能一蹴而就，这个过程有AI的协助，使得在开发的时候无需关注代码怎么写，功能具体怎么实现的，只需要关注接下来需要什么，怎么进行。我深知，整个系统的所有细节我都必须一一把握，进行斟酌，整个系统的更迭必须脚踏实地的进行，功能必须完善，接口必须对齐，容错机制必须跟上，并且在AI协助过程中必须考量修改是否合理，局部的最优解并不一定是全局的最优解，而这是驾驭AI最难的点之一，如何统领整个系统的发展朝预期成长。

整个系统的更迭，大多围绕如何提高审查质量进行，具体的，在这个系统中，核心目标在于如何给审查Agent喂足够好的上下文

个人认为，决定代码审查系统质量最重要的两个变量：
```math
\text{审查质量} = \text{LLM模型能力} \times \text{LLM输入内容}
```

LLM模型能力无法简单的进行归类，通常认为使用最新模型具有一个不错的效果，但目前LLM已经足够支撑大多数代码审查任务，后续如果从此作为切入点提高代码审查质量未尝不可微调一个专门用于代码审查的模型，但受限于实际情况，这个想法的成本远大于收益

因此如何把控LLM的输入内容成为影响审查质量决定性因素，关于如何提升LLM输入质量在后续专门提及，在更迭系统时，按照从简单的反馈系统向复杂的多Agent演进，详见[总规划](核心文档/总规划.md)，整个过程从点到线逐步发展，让输入从diff到diff关联函数可控被选择是否作为上下文被传递，再由线到面，将可能涉及的更多代码进一步被审查。

整个过程也不能简单的认为，传递的越多越好，传递更多上下文如果包含不必要的代码片段，会对审查Agent的输入产生更多信息噪音，同时会消耗不必要的token，如果减少输入，那么可能造成审查不准确，审查Agent频繁调用工具，而调用工具通常认为消耗的token呈线性增长。因此如何把控上下文，是整个系统迭代的核心之一。

### 封装内核

系统内各个模块应该互相信任，向后的链路应该相信前置工作正常运行，就像应用层始终相信API格式是已经处理成私有格式的。

将系统封装为内核的想法是开发过程中根据实际情况产生的，在初期规划中并没有进行相关思考。整个系统的输入为`git diff`+`上下文`，只需要在底层做一个API报错检测机制，就可以认为整个链路正常情况下就会如期运行，无需在系统内添加过多的回退机制，使得系统能够高效率的进行工作。而这，让我更确信前文所说，局部的最优解不一定是全局的最优解，而系统只需对外提供API便可，无需关心外部的实现，做到前后端分离。

## 开发思路

前期确定开发方向确保系统的发展能够按照既定的轨道持续演进，但是如何进行演进成为不可回避的问题，如果说初期的开发方向是确定整个系统的骨骼，那么接下来的开发就是为这个系统构建血肉，让系统能够正确发力，资源得以充分利用。

开发的核心应该落在：合理处理上下文，架构怎么调

这两点决定整个审查系统是否能够产出高质量审查报告，所有工作都应该围绕这两个方向进行展开

### 怎么处理上下文

关于输入给LLM的上下文进行诸多考量，`unidiff`获得的diff视图非常适合人类阅读，可不应该将LLM 视为万能解析器，它理解能力的前提是 **训练语料出现过大量类似格式**，即便LLM绝大多数情况下能够消费此类输入，但会大大增加信息噪音，导致LLM将更多注意力集中在处理格式问题，尤其输入核心内容是diff。

一个有趣的问题，JSON作为一种结构化数据格式，理论上更易于被机器解析和处理，但在[调研](../etc/存档/上下文的权衡/1_DIFF格式.md)过程中发现大部分代码审查系统处于刚起步状态且多为产品，无法了解运行机制，开源产品中大多为基于传统扫描器的代码审查项目，因此我提出一个猜想，JSON格式的数据更适合作为diff输入，同时Markdown作为对diff内容的信息提示。

为了确保审查Agent能够获得高质量上下文，需要进行复杂的动态调配，本系统上下文调度器同时采用**规则层**（基于预设规则快速判断）和**LLM 层**（我称之为决策Agent）共同协作。系统获取diff变更，并使用预设规则为diff进行置信度计算，打上元数据标签，并得到初步JSON上下文，接下来，让决策Agent根据元数据进一步判断这些diff需要什么程度的上下文获取，并输出JSON格式的上下文包，这两个上下文包**基于置信度阈值进行智能融合**高置信度时认为规则拥有绝对的自信，LLM只能提高审查单元，低置信度时规则不再认为对此类变更有自信，此时LLM优先，中等置信度时取上下文级别更高者进行融合，得到的融合JSON将交由系统拉取上下文并传递给审查Agent。

### 选用什么架构

项目初期的构想中原计划使用`langgraph框架` 搭建Agent工作流，这样能够稳定传递各Agent的工作情况，高效的进行审查任务。

但是在实际开发中我并没有采用最初的构思，原因很简单，整个系统或者说代码审查系统并不需要构建复杂的工作流，本系统采用串行，可调配的设计理念进行设计，前置的Agent的结果为后续Agent提供服务，Agent和预设规则协作得到最终上下文包，系统根据上下文包拉取相关上下文传递给审查Agent，整个过程很简单，前置的一切工作只为最后审查Agent服务。

目前策略是Agent作为主链路，静态扫描器作为旁路可选进行，如果将静态扫描器作为前置依赖，为最终的上下文提供更多参考信息或许可以获得更多有价值的参考信息，至于为什么没有将静态扫描器作为前置依赖融入主链路，而是拆分为旁路进行，原因很简单，扫描需要的时间太久，这对于使用者来说通常需要忍受半分钟的空白等待时间，一旦变更的文件数过多，那么这个时间将会长达数分钟，这还是仅考虑扫描器的耗时，接下来链路其他部分根据选用模型不同也将消耗较长时间，显然这对于产品来说是致命的，虽然这个过程可以将后端扫描器的运行结果实时渲染到前端以缓解等待时的焦躁，但依然回避不了审查时间过久的问题。

另外一点，这个过程会经过两个Agent，决策Agent的LLM如果足够强（基本上最新模型都不错），审查Agent对于不确定的判断能够使用工具获取更多的上下文，在这种情况下，扫描器的参与是不是显得不那么重要？为了扫描器提供的边际效应付出更多的时间成本和计算资源是否值得？这有待商榷

> 后续将扫描器结果收敛为工具，审查Agent能够通过工具获取扫描结果

按照最初的思路，后续会增加回归Agent进行统一结果，由回归Agent进行信息统一并做出最后的裁决，可是这个过程没有从实际出发，仅停留在构想中，如果审查Agent能够得到优质的上下文，那么审查报告很能反应代码变更存在的问题，如果使用回归Agent介入，那么结果不出意外总是认为审查报告是准确的，这还是源于LLM的核心——不会主动思考。

因此在后续开发的过程中，如果添加回归Agent的话那么应该进行调整，根据之前的并行链路得到的结果进行归一化，这个Agent应该称之为归一化Agent，遗憾的是，目前已经实现Agent主链路和扫描器旁路，并将结果汇总，能够让归一化Agent消费并进行回归验证，但暂时对归一化Agent暂时没有一个好的想法，并且归一化的实现是否有必要？正如所有代码审查系统遵循的理念一样，AI只是进行辅助，提高效率，人才拥有最终裁决权。

## 开发阻碍

整个项目的开发，往往伴随着功能开发，Bug修复，测试等一系列过程，坦白的说，越到后期，整个项目我能手动调整的地方越少，一个不小心就会导致大量错误，尤其是系统内核的调整，系统内核运行遵循相互信任的逻辑，一旦修改需要考虑数据传递、阻塞进程、上下游模块的兼容性等诸多问题。而借助AI进行开发虽然效率极高，但AI并不真正理解整个系统的全貌，它只能看到当前对话窗口内的上下文，这就导致一个尴尬的局面：越是复杂的修改，越需要开发者自己把控全局，AI进行的修改需要开发者自己反复斟酌。

另一个阻碍来自于调试。当系统出现问题时，排查往往需要追溯整条链路，从diff解析到规则匹配，从决策Agent到上下文调度，再到最终的审查Agent，任何一个环节的异常都可能导致最终输出不符合预期。而这个过程，需要细致分析，排查日志，去诊断哪里出现问题，即便有AI的加持，也常常无法正确切中问题，需要自己进一步分析数据链路并进行分析。

当然，有了AI后，一切都变得可控起来，对于任何具体的问题，总能通过AI获得一个不错的反馈，其中包括功能开发，UI设计，架构调整等等，可有AI，但如何使用AI也是非常无奈的一点

自认为整个项目算规模较大，架构复杂的系统，从零开始构建到最后落地运行的过程中全程有AI影子，可以说，只要IDE窗口打开，那么AI就在运行，保守估计整个项目的开发消耗`5亿`token，测试消耗`1`token，但得益于不同厂商的慷慨，使得开发不受AI API掣肘，尤其以Costrict校园挑战提供4000Credits和GitHub Copilo每月学生包，Codex高额度，Trae900次使用权益，Kiro的1500Credits，Antigravity每五小时刷新速率限制，Windsurf Pro提供的Free模型让整个项目的开发如期进行

在实际使用，每一次功能的开发和链路测试上，均会反复调整，确保功能正确，上下模块能正常消费，不过这个过程也并不总是一帆风顺，受限于自身水平，对于具体需求无法很好的创建提示词，因此在真实开发中需要不断进行追问进行调优，有时陷入混乱的思维，将代码修改的一团糟，庆幸于在每次功能开发后及时使用git暂存文件，使得每次糟糕的变更能够回溯最初的模样。

有了AI后，尤其是Claude，对于开发过程中的难点并不需要我着手处理，我只要提供需求就行，整个过程会遇到各种各样的问题，和各种意外情况，即便能够通过AI解决，但是这个过程依然艰难，我将这个过程称为熬。从项目创建之后，每一次优化，迭代，测试都需要我亲自规划，这个过程AI无法完成，只能协助，对于一些隐蔽的问题，AI无法察觉，只能在测试中发现，如果测试也没发现，那么这将成为一个潜在Bug。

# 一些实现

由于整个链路采用复杂的结构进行协同工作，对于技术实现参见[技术详解](技术详解)，系统介绍参见[核心文档](核心文档)，在此只介绍部分功能实现

## 审查如何贴合项目业务意图

> 代码审查不是孤立的，它必须先理解业务意图

和这个项目一样，其他项目也有此类的技术文档与README.md，这写文档构成整个项目开发的指南，使得开发者能够按照规划进行项目的推进，和其他项目不同的是，本项目旨在针对其他项目进行代码审查，为其他项目的开发提供服务。因此，如何理解其他项目的开发理念和业务意图至关重要，进行代码审查需要确定整个项目是要干什么，否则审查Agent将会变得茫然，只会针对修改的修改的代码泛泛而谈。

因此在正式进行上下文拉取前，需要确定这个审查的项目是一个什么项目，它的业务意图是什么，基于这个点，为这个系统新增一个分析Agent，通过获取项目文件树，`Markdown文件`和git提交信息等相关数据作为分析Agent消费信息，分析Agent的结果将向后续的决策Agent和审查Agent提供审查方向。

关于如何开展贴合项目业务意图进行进一步分析，在后续还可以继续优化，目前实现方式很简单，效果很大程度取决于READM.md和git的提交信息，通常认为一个优秀的项目具备良好的设计规范和优秀的文档，基于这个角度考虑，目前的业务意图分析对完善项目应有一个不错的效果。

如何提高业务意图分析，可考虑使用MCP，RAG知识库等技术进行进一步优化，使用这些方式能让分析Agent得到的业务意图分析更加准确且细致，但是本系统开发时间不足以支撑再为此方向继续优化，值得注意的是，即便无法使用分析Agent获得一份十全十美的业务意图分析报告，但用户仍能手动修改报告进行微调，这些微调的报告将会如期的传递给后续链路

## 自我学习机制

> 一个好的系统不应该止步于完成任务，它应该在运行中不断学习、不断成长。

系统应该始终相信通过规则得出的上下文信息是必要的，LLM反馈的上下文信息是符合整个审查预期的，但这两者之间难免会出现分歧，这种分歧在融合阶段会被处理掉，但分歧本身不应该被忽略。规则层认为某个变更只需要diff_only，LLM却觉得需要更多上下文；规则层给出高置信度，LLM却选择跳过，说明规则可能**过度敏感**，对无关紧要的变更也要求审查。规则层不确定（低置信度），但 LLM 给出了明确建议，这类冲突可用于**发现新的规则模式**。规则层和 LLM 层建议的上下文级别差异较大等等，这些冲突都会被系统一一记录并统计。

自我学习机制是代码审查内核的核心创新之一，通过追踪规则层与LLM决策之间的冲突，自动提取可复用的规则模式，当某一类冲突达到一定阈值，系统便可自动应用为基础规则并在接下来的代码审查中应用此类规则，同时开发者可查看所有冲突规则，主动删除已应用规则或将未达到阈值的规则进行主动应用，进而实现规则库的持续优化。

## 前端优化

> 影响使用体验的不仅仅是审查效果，使用过程同样不可忽视

如果说整个内核经过深思熟虑后一步步进行开发和升级，那么前端的诞生则更加"野蛮生长"——没有经过细致的规划，而是在不同交互方案上反复测试，做到怎么舒服怎么来。这种开发方式看似随意，但始终遵循一个基本的原则：**用户体验优先**。

前端开发初期，所有代码都堆积在`main.js`和`style.css`中，随着功能迭代，这两个文件分别膨胀到6700行和4500行。代码的臃肿带来了维护的噩梦——每次修改都如履薄冰，牵一发而动全身，这在后期的开发中变得寸步难行。

很显然，这样的开发定会让项目的开发变得困难，因此在项目中期对此进行重构，模块化的好处立竿见影：代码可读性大幅提升、功能迭代变得轻松、Bug排查也有迹可循。

前端的开发绝不比内核的开发轻松，在大多数时候，需要站在用户角度去尝试、去犯错，为每种可能存在的错误情况设计不同的处理措施。在此基础上还需要做到高质量的UI、流畅的动画、自然的交互等等，这导致和内核开发完全不一样的开发体验——内核追求的是逻辑的严密和数据的准确，而前端追求的是感受的舒适和操作的自然。

在交互细节上，诸如LLM输出时右侧面板的智能滚动、折叠面板的状态保持、实时进度的流式渲染等，都经过反复调试。或许一个看似简单的自动滚动功能，按钮样式设计，审查报告映射等诸多细节都经过长期反复打磨，即便有`gemini 3 pro`在面对负复杂的前端也常显得羸弱，很多时候需要靠自己找到确切代码段并进行提示词构建。

整个前端的设计竭尽全力做到尽善尽美，但我知道里面还会有很多冗余代码和一些不太自然的交互设计，但是我还是花费不少时间仔细优化，如果存在些许错误还望海涵。



# 后记

从最早的石器到如今的大语言模型，工具的形态在变，但本质从未改变——它们都是人类意志的延伸。石器延伸了手，轮子延伸了脚，文字延伸了记忆，而AI，延伸的是思维本身。

AI可以写出比我更规范的代码，可以给出比我更全面的方案，可以在几秒钟内完成我几个小时才能做完的工作，但它永远不会问自己："这个系统应该是什么样的？"它只会问："你想让我做什么？"

主动思考的能力，是人与工具之间那条永远无法跨越的界限。

整个项目的开发过程中，AI完成了99%的代码编写，但那1%——决定写什么、为什么写、往哪个方向写——始终握在我手里。这1%直接决定这个项目的发展方向和成败。

这个项目用Python编排工作流，实现各种功能，但同样可以用C或C++完成。选择不同的语言，就像选择不同的工具来完成既定的需求。AI也是如此，学会调度不同的模型完成不同的任务，何尝不是一种能力？就像学习编程语言一样，知道什么场景用什么模型，什么任务交给什么AI，这本身就是一个需要学习的过程。
