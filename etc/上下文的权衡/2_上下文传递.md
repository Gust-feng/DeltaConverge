# 2_上下文传递：从“堆消息”到“分层记忆”

> 目标：从现在的“线性叠加所有历史消息”升级为**分层、可控、可裁剪**的上下文传递体系，在不丢关键信息的前提下，降低噪音和 token 消耗，并为未来多轮对话留出空间。

---

## 一、现状与问题

### 现状（v1.5）

- `ConversationState` 只是一个简单的 `messages: List[Dict]`：
  - 每次调用 `CodeReviewAgent.run`：
    - 如果是首次调用，会 append 一条 `system` 消息。
    - 构造一条包含「用户 prompt + diff 上下文（现在是 Markdown+精简 JSON）」的 `user` 消息。
  - 在工具轮回中：
    - 每轮 append 一条 `assistant` 消息（包含 content + tool_calls）。
    - 对每个工具结果 append 一条 `tool` 消息（content 为工具输出全文）。
  - 下一轮调用 LLM 时，`self.state.messages` 整包原样发给模型。
- `ContextProvider` 只按字节数截断项目文件内容，和对话历史无关。
- `ConversationState.set_history_limit / prune_history` 已实现，但目前没有在任何地方被调用。

### 问题

1. **消息线性增长，无任何清洗**
   - 所有历史消息都被完整保留，且每轮请求都重新发送。
   - 工具调用多、输出长时，历史呈指数级放大（尤其是 `list_project_files`、`search_in_project` 这类工具）。
2. **不同类型上下文混在一起**
   - 系统提示、diff 上下文、对话轮次、工具输出统统堆在同一个 `messages[]`。
   - 无法针对“稳定上下文”和“临时上下文”采用不同策略。
3. **没有 token 预算和“价值排序”**
   - 没有任何 token 预算控制，也没有“优先保留哪些信息”的规则。
   - 一旦支持真正的多轮对话，很快会遇到：
     - 被模型/服务端硬截断（丢信息不可控）。
     - 旧的工具结果、旧的 diff 解释持续对当前判断产生噪音。

---

## 二、设计目标

1. **分层上下文**：区分「静态上下文」「短期对话」「长期摘要」「工具记忆」四类内容。
2. **受控增长**：对任一层都有明确的上限（消息数 / 字符数 / 近似 tokens）。
3. **价值优先级**：优先保留高价值信息（决策、结论、关键信息），压缩或丢弃低价值信息（冗长工具输出的细节）。
4. **渐进可落地**：先在当前单轮审查场景上实现可控的“短期窗口 + 工具压缩”，再逐步演进到多轮对话与摘要记忆。

---

## 三、上下文分层模型（目标结构）

面向 LLM 的整体上下文可以拆成四层：

1. **静态上下文层（Stable Context）**
   - 固定系统提示（system）：
     - “你是 AI 代码审查员……遵循 Moonshot 工具调用规范……”
   - 当前审查任务的“稳定项目视图”：
     - Diff 视图：`build_markdown_and_json_context` 生成的 Markdown+精简 JSON。
     - 可选文件片段：`ContextProvider` 选出的关键文件 snippet。
   - 特点：
     - 对当前任务而言是常量，不随每一轮工具调用而改变。
     - 不应该被重复叠加；多轮对话时同一份上下文只需出现一次。

2. **对话窗口层（Short-Term Dialogue Window）**
   - 最近若干轮 `user/assistant/tool` 消息：
     - 用户追加的问题 / 补充说明。
     - 模型的回答、思路解释。
     - 最近一两轮工具结果（经过压缩）。
   - 这是“滚动窗口”，长度受限（按轮数或近似 token 控制）。

3. **长期摘要层（Long-Term Summary）**
   - 当对话轮次较多、历史过长时，将较早的对话压缩成一条或少数几条“摘要消息”：
     - 概括已经讨论过的结论。
     - 标注已经做过的检查、已确认的假设。
   - 模型再次推理时可以依赖这些摘要，而无需重读所有原始对话片段。

4. **工具记忆层（Tool Memory）**
   - 将工具的“完整结果”从对话流中剥离出来，单独缓存：
     - 对话里只保留“此工具做了什么 + 简要结果摘要 + 引用 ID”。
     - 需要细节时，让 LLM 再通过工具调用或引用 ID 访问完整内容（例如：`read_file_hunk` 按需再读）。
   - 对于本项目，适合作为 Tool Memory 的工具包括：
     - `list_project_files`
     - `search_in_project`
     - `get_dependencies`
     - 大块 `read_file_hunk`

---

## 四、消息管理策略（设计稿）

### 4.1 ConversationState：从“列表”到“窗口管理器”

在现有 `ConversationState` 基础上增加：

- **逻辑分组 / 回合概念**（后续可迭代）：
  - 一次用户提问 + 若干轮工具调用 + 最终回答 = 一个“审查回合”。
  - 未来可以用 `turn_id` 或简单计数区分不同回合。
- **窗口限制参数**（可配置）：
  - `max_turns`: 最多保留多少最近回合（对短期窗口）。
  - `max_messages`: 硬上限，作为安全兜底。
  - `max_tool_chars_per_message`: 每条 tool 消息最多保留多少字符，多余部分用摘要替代。
- **prune_history 策略调整**：
  - 当前版本：简单按条数截断。
  - 目标版本：
    - 优先保留：
      - system + 当前任务的静态上下文（永不裁剪）。
      - 最近 N 轮 user/assistant 消息。
      - 最近几条重要 tool 摘要。
    - 优先裁剪：
      - 旧的 tool 细节（保留“摘要 + 引用 ID”）。
      - 重复的上下文（例如多次重复贴 diff）。

### 4.2 工具输出的压缩与替代

对于 `add_tool_result` 目前直接使用 `result["content"]` 的行为，计划优化为：

1. **在线压缩**：
   - 对超长内容（超过 `max_tool_chars_per_message`），只保留：
     - 开头几行 + 结尾几行。
     - 行数 /结果条目数量等统计信息。
     - 一个“引用句柄”（例如 `tool_result_id` 或 `file_path + hunk_range`）。
   - 在 `content` 中明确写出：
     - “完整结果已截断，如需详细内容，请通过工具 XXX with 参数 YYY 再次获取。”

2. **外部缓存（Tool Memory）雏形**：
   - 后续可以在 `ToolRuntime` 或独立模块中维护：
     - `{tool_call_id -> full_result}` 的映射。
   - 对话消息只存摘要，而不是原始大块文本。

### 4.3 Diff 上下文与对话历史分离

目前在 `CodeReviewAgent.run` 中，每次调用都会把：

```text
prompt + Markdown diff 上下文
```

合成到一条 user 消息。这在“单次运行”的情况下没有问题，但在多轮对话时会导致同一 diff 被反复叠加。

优化思路：

- 把 diff 上下文放入一个单独的 system 或 pseudo-system 消息：
  - 例如 role 可以是 `"system"` 或 `"user"`，但标记为“静态上下文，不随回合重复”。
- 之后用户追加提问时，只发送新的 user 文本，不再重复整个 diff。
- 在 `ConversationState` 中，可以专门标记这类“固定上下文消息”，在 `prune_history` 时永不裁剪。

---

## 五、LLM 调用时的“上下文打包算法”（v1 方案）

每次调用 LLM 前，构造 `messages` 建议采用如下顺序和策略：

1. **必选层**
   - 1 条系统提示（固定）。
   - 1 条静态上下文（diff Markdown+精简 JSON；或拆成 1~2 条消息）。
2. **长期摘要层（若存在）**
   - 0~2 条摘要消息，用于概括早期多轮对话的结论。
3. **短期窗口层**
   - 最近 N 条 `user/assistant/tool` 消息（按轮数/最近时间排序），N 由近似 token 预算控制：
     - 粗略估计：`len(content)` × 系数 ≈ token 数。
     - 或未来根据 provider 的 `usage` 信息动态调整。

> 关键点：**在打包阶段统一裁剪，而不是在消息写入阶段就丢掉信息。**  
> 这样可以根据不同模型/场景调整“可见窗口”的大小，而不影响底层存储。

---

## 六、渐进落地计划

为了不一次性把系统搞得太复杂，可以分阶段实现：

### v1：受控窗口 + 工具输出压缩（近期可以落地）

- 在 `CodeReviewAgent.run` 初始化时：
  - 调用 `state.set_history_limit(max_messages=K)`，例如 `K=40`。
- 在 `add_tool_result` 中：
  - 对超长 `content` 进行截断，追加一条“已截断说明”。
- 在每轮调用 LLM 前：
  - 显式调用 `state.prune_history()`。

> 这一阶段主要减缓线性爆炸，简单但有效。

### v2：静态上下文与对话历史分离 + 简单摘要

- 将 diff Markdown+JSON 独立成一条“静态上下文”消息，只在首次 run 时注入。
- 对早期多轮对话，尝试用简单规则或 LLM 自身生成一条摘要消息，并删除对应的原始长消息。

### v3：引入 Tool Memory & 更精细的价值排序

- 在 ToolRuntime 增加一个可选缓存层：
  - 记录完整工具结果；对话中只保留摘要 + 引用 ID。
- 在构建上下文时，基于：
  - 变更 tags（安全 / 配置 / 逻辑等）、
  - metrics（规模 / 复杂度）、
  - rule_suggestion / agent_decision，
  对历史消息做“价值排序”：优先保留高风险、高复杂度相关的对话和工具结果摘要。

---

## 七、小结

当前阶段，diff/ReviewUnit/JSON 的设计已经处于领先水平，真正的瓶颈转移到了“如何把这些信息以**高价值、低噪音、可持续**的方式交给 LLM”。  

这份“2_上下文传递”的规划，意在：

- 不推翻现有结构，而是在 `ConversationState + CodeReviewAgent` 的基础上，增加一层“上下文调度策略”；
- 先解决明显的线性增长和工具噪音问题，再逐步把对话历史演化为有摘要、有优先级、有记忆层次的上下文系统。

后续可以在此文档基础上，把每个阶段拆成具体的开发任务与测试场景。  

