# 优化方向

## 做上下文自适应规划

Agent 在开始审查前，先进行一次“审查规划”。

类似于：

先判定 PR 类型

- 确定需要的上下文类型

- 决定要不要扩展上下文（工具调用）

- 给每种上下文分配 tokens 预算

- 确定优先检查哪些模块（安全/逻辑/一致性）

这个动作就是：

自动规划上下文范围

## 把“上下文”从文本升级成“多维审查视图”

现在大部分系统都基于：

`“上下文 = 代码文本 + diff 文本 + prompt 文本”`

我在想能不能在此基础上优化为LLM更好处理的上下文结构：

上下文可以是：

依赖图

调用链

不变量（invariants）

数据流

控制流切片

风格约定

风险标签

规则解释

让模型不是在“读代码”，
而是在“读项目结构”。

## 分模块上下文

传统上下文都是“一坨”。

可以把上下文拆成模块包：

{
  "diff_context": [...],
  "function_context": [...],
  "call_chain_context": [...],
  "config_context": [...],
  "security_context": [...],
  "tests_context": [...],
}


模型读的时候会非常聪明，因为这些模块让它“懂得自己在看什么”。

这也是创新：
上下文不是扁平的连续文本，而是结构化的输入维度。

## 上下文的“动态扩张” vs “动态收缩”

模型遇到某个疑点 → 自动展开上下文 → 再缩回去

比如：
模型看到 SQL 查询 → 自动展开 ORM model / DB schema → 审查完再收缩为摘要。

## 上下文的“解释层


模型不是只读代码，而是读“解释过的代码”。

例如规则层生成：

“这个函数是幂等设计”

“这个模块有安全敏感标签”

“这个 diff 改变了数据流”

也就是把：

`context = code + metadata + reasoning hints`

## 上下文的“生命周期管理”

传统上下文都是静态的。

能否做：

初筛阶段上下文（粗）

深度审查上下文（宽）

最终总结上下文（抽象）

每个阶段上下文的结构不同。

## 主动生成“上下文假设

模型先猜测：

改动可能影响哪些调用者？

可能破坏哪些不变量？

哪些模块可能隐含 bug？

然后根据这些“假设”去决定：

该展开哪些文件

该跑哪些工具

该优先检查哪些路径

# 需要思考的点

## 上下文的“度量体系”还没有出现

现在提出了依赖图、调用链、动态扩张、解释层、多维上下文，但还缺了一样东西：
 **这些上下文产生的“价值感知机制”。**

换句话说：
 模型怎么知道哪些上下文值得展开？
 展开多少？
 什么时候停止？
 展开后是否真的提高了审查质量？

没有“上下文价值度量”，系统会变成一台很聪明但没规则的液压机：强，但不稳。

**缺什么？**

缺一个“上下文评分（Context Score）”体系，用来给每个候选上下文加权，例如：

- 变更相关度（relevance）
- 风险等级（risk_score）
- 复杂度（cyclomatic complexity）
- 依赖影响范围（impact span）
- 内容密度（information density）
- 涉及未变更区域的逻辑复杂度（non-local impact cost）

## 上下文丰富，但并没有描述如何保持“稀疏性”

提出的多维上下文真的非常强，但这个系统如果直接实现，会迅速发生：

- 每个 ReviewUnit 都携带巨量属性
- 上下文在多维度叠加后呈指数增长
- 模型面对 5 万 tokens 的 JSON，不会变聪明，只会变茫然

说白了：

> 上下文维度很多，但没有“稀疏化策略”。

没有稀疏策略，你就会遇到“好情报太多等于无情报”。

应该需要：

- 维度筛选（dimension pruning）
- 信息压缩（context summarization）
- 稀疏矩阵式表示（sparse context mapping）
- 基于查询语义的 relevancy pruning（语义裁剪）

## **没有“项目状态模型（Project State Model）”**

你提出了调用链、依赖图、不变量，这些其实都属于“项目的静态状态”。

但你没有描述：

> 这一切如何被缓存、更新、增量构建？

比如：

- 当项目变更非常多时，你不可能每次都重新解析全局依赖关系
- 多个 PR 同时到来，系统如何知道状态是 A、B 还是 C？
- 一个文件被删了，所有引用图如何更新？

没有“项目状态模型”，你的上下文再强也会成为一次性计算。



## **Diff 的内部结构很好，但缺少“审查行为层”**

第二份文件（diff.md）研究的是：

- 输入格式
- 模型能不能吃
- 模型吃得好不好
- 外部 patch vs 内部 JSON

但你缺少一层：

> 模型应该怎么“使用”这些结构？
>  它的审查行为模式是什么？

缺什么？

- 错误优先级排序（severity ranking）
- 审查策略（analysis modes）
- 风险类型→审查步骤映射（risk→checklist）
- 冲突生态（例如安全 vs 性能 vs 清晰性）

## **没有“失败回退机制（fallback plan）”**

这是所有 AI 系统真正的死亡点。

例如：

- 上下文扩展太大 → 模型超载
- search-replace 匹配失败 → patch 应用失败
- 模型审查错误 → 需要回滚到上一层
- 决策 Agent 推错路径 → 需要 checkpoint
- 自动修复导致引发更多错误 → 需要 revert

你现在提出的是“扩张、收缩”，但不是一种强制性的“可逆机制”。

没有 fallback 的系统不会可靠。

## **缺乏“自我一致性机制”**

任何高端审查系统都需要一种 internal consistency check，例如：

- 逻辑一致性检查
- diff 和解释中的矛盾检测
- 风险评估与审查意见之间的矛盾检测
- patch 与上下文分析的一致性验证

你现在的设计非常像“一个很厉害的单 Agent”，
 但还没有“多 Agent 互相校验”的机制。

## **没有定义 tokens 成本模型**

多维上下文、动态扩张、结构化 diff、JSON 语义结构……

这些都会带来 tokens 成本爆炸。

未来如果模型变贵或团队规模变大，你必须考虑：

- 单次审查的 tokens 预算
- 预算不足时的裁剪策略
- 预算过度时的风险
- 如何对上下文进行层级压缩
- 哪些上下文必须进入模型，哪些不需要

这是一个缺失的核心模块。