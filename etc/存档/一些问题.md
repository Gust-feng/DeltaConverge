# 上下文处理

## 2025-11-19

* 随着本地维护上下文消息，每次调用API导致tokens线性增长，尤其是工具使用上的增长直接导致tokens的消耗按工具调用倍数增长，于此同时我注意到即便LLM支持同时调用多个工具，但实际运行时，LLM倾向于单独调用工具，以至于成功调用工具后需要重新发起新的API请求，这不仅大量消耗tokens的同时还会造成不必要的浪费

* 对于上下文的长文本输入，LLM注意力机制会出现["中间迷失"](https://arxiv.org/html/2508.05128v1)和["注意力盆地"](https://arxiv.org/html/2508.05128v1)

* [x] 目前上下文体系还是“最简单、但浪费”的实现——它能保证语义正确（不丢信息），但从“噪音控制”和“tokens 成本”来看，是明显需要优化的

  > 上下文使用`规则+LLM=上下文进行调度

* [x] 目前diff片段使用的是修改前的上下文+修改后的上下文进行拼接，显然这是不合理的，仅需要将修改的diff和公用的上下文进行拼接

​	`DIFF/diff_collector.py` 生成的 unified_diff 只包含修改行和必要的上下文行，不会把修改前/后的完整文本双倍塞入。

* [x] 仅支持工具调用间的上下文传递，未支持多轮 Agent 调度时的全局状态管理。

## 2025-11-26

* [x] 规划输出未做 Schema 校验，出现 `"extra_requests [],": "kip_review"` 等脏字段仍被接受；需要按 JSON Schema 过滤/修正非法键。

  > 对提示词进行调整；对LLM返回结果进行过滤/修正

* [x] 融合阶段未真正筛选，`fuse_plan` 仍对全部 review_index.units 生成 plan，planner 的选中/去噪失效；应只保留“planner 选中 + 规则高风险/高置信度兜底”，其余默认 skip_review。

* [x] 上下文调度存在安全隐患：`_git_show_file`/`_search_callers` 直接拼接子进程参数，缺少路径/符号白名单，需改为安全调用（规范化路径，拒绝 `..`/重定向/空格注入，或使用 `git show -- <path>`）。

* [x] 规则层异常时 `rule_context_level` 可能为 unknown，融合/调度阶段会当正常值用；应在融合阶段兜底为 diff_only 并记录 warning，方便排查规则错误。

* [x] 可观测性不足：未记录实际下发给 LLM 的上下文大小/截断信息；`build_context_bundle` 后应写入各级上下文字符/行数、截断/降级次数。

  > 增加回退观察机制

* [x] CodeReviewAgent 会话日志初始化缩进异常（首轮 `_trace_logger.start` 代码嵌在 `if not self.state.messages` 内），可能导致首轮漏记日志；需要整理缩进/结构。

* [x] 规划/审查失败兜底不足：planner timeout、JSON 解析失败或 LLM 错误时，应在 pipeline 日志写明并返回可解析的空计划/错误计划，避免后续链路级联异常。

## 2025-11-26

- [x] 现阶段每轮使用工具导致tokens消耗量剧增，平均10个文件，25-30变更消耗35万tokens。其中大部分由工具调用返回的结果消耗且呈现线性增长，是否考虑进行针对调用工具进行优化。

  > 优化首轮文件树，减少LLM不清楚项目层级递归获取文件路径产生不必要请求

# 模型选择

## 2025-11-20

我发现`GLM-4.6`似乎钟情于调用工具，以至于一次简单改动居然打掉十万tokens

> 当LLM尝试递归读取文件路径，产生的token甚至达到百万，在首轮提示词中加入文件树后消耗的token大大降低

## 2025-11-22

我对`GLM-4.6`,`kimi-k2-0905-preview`,`qwen3-max`进行测试，发现一个问题，似乎不同的模型对工具调用的偏好是不一样，在实际使用中`GLM-4.6`偏爱调用工具，而其他两款模型似乎不太愿意主动调用工具，并且检查具体实现并测试后这两个模型能够调用工具，只是为什么相比`GLM-4.6`调用工具差这么多

### 应对策略考量

是否将工具调用权从模型迁移到Agent的显示规划中？

# 工具兼容性

* [x] 调用工具时，如果不启用 auto_approve 且审批拒绝执行工具时，不会给LLM返回拒绝信息；
  
  > 已在 `Agent/agent/agents/code_reviewer.py` 中改为生成错误工具结果，模型能收到明确拒绝原因，避免无限请求

# 规则解析

- [x] 当前规则解析有点简单，置信度打的有点糊涂，且常返回unknown，并且需要为五种语言做专门适配及优化

# UI设计

- [x] 获取思考模型思考结果时，发现只支持整体流式返回

# 扫描器

初步想法是为每个语言都安装扫描器，但是需要配置好对应环境，而本地并没有配置好这些环境，所以选用`semgrep`作为通用扫描器，不过这个扫描器的耗时相对久，并且和其他扫描器相比会不会功能重叠了？

在对扫描器进行进一步开发过程中发现一个严重的性能问题，因为扫描器的初始化导致在构建发送给决策agent的元数据之前需要半分钟的时间，显然这是非常糟糕的，对于用户而言，需要经历半分钟的空白等待时间，虽然可以将扫描器初始化进度实时渲染前端，但是我在想为了给决策agent提供更多的元数据，这种边际效用是否必要，尤其对于变更较小的pr是否显得复杂，这个有待商榷

## 2025-12-08

在每次审查前，扫描器需要经历长达半分钟的扫描，这对整个链路造成严重的影响

现在的扫描器调用方式有点问题，逻辑上看是为每一个变更单元进行审查，但这导致文件扫描不合理，更优解是，先根据变更内容计算一个待扫文件列表（去重 + 按风险排序：安全/配置/大改动优先）；让扫描器按照文件任务为维度跑（可以考虑为每一个文件配置一个开始/结束事件）



## 2025-12-09

 扫描器应该是**增强**，而不是**前置硬依赖**，一来扫描器归一提供的元数据对决策Agent来说并不是必要的，二来将扫描器作为**前置扫描器 **需要足够的时间，这就意味着在扫描没跑完前，规则建议出不来，规则建议出不来，Planner的输入不完整，前端需要忍受很长的空白窗口期间，虽然这个空白窗口期可以通过将扫描器扫描情况实时渲染进行缓解，但这是不合理的

更好的思路是，按照先前的链路跑一遍，扫描器作为**异步补充**

由点到面，扩展思路下：

先确定核心思想**数据流以diff为中心，以LLM为大脑，扫描器进行辅佐**

深度扫描：使用扫描器作为前置扫描，为决策agent提供足够的元数据

标准扫描：扫描器只作为旁路进行扫描

轻量扫描：扫描器不参与扫描

| 模式 | 扫描时机         | 是否阻塞主链路       | 决策 Agent 是否依赖 | 审查 Agent 是否用  | 适合场景      |
| :--- | :--------------- | :------------------- | :------------------ | :----------------- | :------------ |
| 轻量 | 不跑             | 否                   | 否                  | 否                 | 快速 review   |
| 标准 | 与主链路并行异步 | 否                   | 否（首轮）          | 可选补充           | 日常 PR       |
| 深度 | 先扫描后决策     | 是（受用户知情同意） | 是                  | 是（重点解释问题） | 安全/关键分支 |

> 深度扫描是目前版本的链路

而基于标准模式下，可以再提出一个思路，构建一个归一化Agent，根据两个结果进行审查，但这是否显得复杂，这有待商榷

这个过程会经过两个agent，决策agent的llm如果足够强（基本上最新模型都不错），审查agent对于不确定的判断能够使用工具获取更多的上下文，在这种情况下，扫描器的参与是不是显得不那么重要？反而，我觉得将扫描器作为单独的信号，为归一化agent（设想），或审查agent的输入是否更有价值？

基于上述考虑，三种模式其实可以统一为 标准模式 PRO版本

> 扫描器的消费无非在
>
> 1. 作为决策 Agent 的输入；
> 2. 作为审查 Agent 的输入；
> 3. 作为独立信号，给“归一化 Agent / 结果对比层”。

做到这里，又可以提出一个思路，配置一个工具链，让审查agent决定是否调用这个工具链，而这个工具链包括扫描器

# 2025-12-12

重新调整链路，将扫描器作为旁路进行工作，不参与主链路，后续若想使用深度扫描策略可重新调回，但应该作为一个可选功能，而不是主链路